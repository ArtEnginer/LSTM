{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQM08 Real Power Mean (kW)       0\n",
      "RmTmp - DH01_ROW01_LA_TTHT01     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT02     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT03     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT04     0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT01    0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT02    0\n",
      "dtype: int64\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load data from Excel file\n",
    "data = pd.read_excel('dataset/dataset.xlsx')\n",
    "\n",
    "# Parse timestamps and set as index\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Select relevant features\n",
    "features = ['PQM08 Real Power Mean (kW)', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT01', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT02', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT03', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT04', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT01', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT02']\n",
    "\n",
    "data = data[features]\n",
    "\n",
    "# preprocess data to remove NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "print(data.isnull().sum())  # Untuk mengecek jumlah nilai NaN\n",
    "print(np.any(np.isnan(scaled_data)))  # Untuk mengecek apakah ada nilai NaN setelah normalisasi\n",
    "print(np.any(np.isinf(scaled_data)))  # Untuk mengecek apakah ada nilai infinity setelah \n",
    "\n",
    "\n",
    "# Print first 5 rows of scaled data\n",
    "print(scaled_data[:5])\n",
    "\n",
    "# Tentukan panjang data latih (misalnya 80% dari total data)\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "test_size = len(scaled_data) - train_size\n",
    "\n",
    "# Bagi data menjadi data latih dan data uji\n",
    "train_data = scaled_data[0:train_size, :]\n",
    "test_data = scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "# Langkah 2: Memformat Data untuk LSTM\n",
    "# LSTM memerlukan data yang diformat dalam bentuk tiga dimensi: (jumlah sampel, jumlah time steps, jumlah fitur).\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), :]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + time_step, 1])  # Memilih kolom suhu sebagai target\n",
    "    return np.array(data_X), np.array(data_Y)\n",
    "\n",
    "# Menyiapkan data latih untuk model\n",
    "time_step = 10  # Jumlah time steps\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input untuk LSTM [jumlah sampel, time steps, jumlah fitur]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "# Inisialisasi model LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], len(features))))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Kompilasi model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Melatih model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluasi model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f'Training loss: {train_loss}')\n",
    "\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQM08 Real Power Mean (kW)       0\n",
      "RmTmp - DH01_ROW01_LA_TTHT01     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT02     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT03     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT04     0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT01    0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT02    0\n",
      "dtype: int64\n",
      "False\n",
      "False\n",
      "[[0.34869488 0.14371257 0.08985025 0.05882353 0.34660422 0.27756131\n",
      "  0.29310734]\n",
      " [0.35183535 0.14371257 0.08153078 0.05294118 0.34660422 0.27549851\n",
      "  0.2980791 ]\n",
      " [0.34450742 0.13972056 0.08485857 0.0745098  0.34660422 0.27045611\n",
      "  0.28497175]\n",
      " [0.34964632 0.16167665 0.08985025 0.0745098  0.34660422 0.2622049\n",
      "  0.28610169]\n",
      " [0.36538433 0.14371257 0.08485857 0.06862745 0.34192037 0.26953931\n",
      "  0.29220339]]\n",
      "Epoch 1/100\n",
      "732/732 [==============================] - 18s 15ms/step - loss: 4.3872e-04\n",
      "Epoch 2/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.9559e-04\n",
      "Epoch 3/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.8543e-04\n",
      "Epoch 4/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.8191e-04\n",
      "Epoch 5/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.7811e-04\n",
      "Epoch 6/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.7679e-04\n",
      "Epoch 7/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.7019e-04\n",
      "Epoch 8/100\n",
      "732/732 [==============================] - 17s 23ms/step - loss: 1.6748e-04\n",
      "Epoch 9/100\n",
      "732/732 [==============================] - 19s 27ms/step - loss: 1.6832e-04\n",
      "Epoch 10/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.6561e-04\n",
      "Epoch 11/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.6474e-04\n",
      "Epoch 12/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.6507e-04\n",
      "Epoch 13/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.6341e-04\n",
      "Epoch 14/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.5966e-04\n",
      "Epoch 15/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.6383e-04\n",
      "Epoch 16/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.6016e-04\n",
      "Epoch 17/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.5893e-04\n",
      "Epoch 18/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5844e-04\n",
      "Epoch 19/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.5687e-04\n",
      "Epoch 20/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5835e-04\n",
      "Epoch 21/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5631e-04\n",
      "Epoch 22/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5442e-04\n",
      "Epoch 23/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.5536e-04\n",
      "Epoch 24/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5583e-04\n",
      "Epoch 25/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5296e-04\n",
      "Epoch 26/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5123e-04\n",
      "Epoch 27/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5385e-04\n",
      "Epoch 28/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5015e-04\n",
      "Epoch 29/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5327e-04\n",
      "Epoch 30/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5163e-04\n",
      "Epoch 31/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.5188e-04\n",
      "Epoch 32/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.5172e-04\n",
      "Epoch 33/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4922e-04\n",
      "Epoch 34/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.5065e-04\n",
      "Epoch 35/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5291e-04\n",
      "Epoch 36/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4966e-04\n",
      "Epoch 37/100\n",
      "732/732 [==============================] - 10s 14ms/step - loss: 1.4971e-04\n",
      "Epoch 38/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5294e-04\n",
      "Epoch 39/100\n",
      "732/732 [==============================] - 10s 14ms/step - loss: 1.4862e-04\n",
      "Epoch 40/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4938e-04\n",
      "Epoch 41/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4932e-04\n",
      "Epoch 42/100\n",
      "732/732 [==============================] - 10s 14ms/step - loss: 1.4955e-04\n",
      "Epoch 43/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4819e-04\n",
      "Epoch 44/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.4777e-04\n",
      "Epoch 45/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4930e-04\n",
      "Epoch 46/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4841e-04\n",
      "Epoch 47/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4774e-04\n",
      "Epoch 48/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4666e-04\n",
      "Epoch 49/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4929e-04\n",
      "Epoch 50/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4623e-04\n",
      "Epoch 51/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4777e-04\n",
      "Epoch 52/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4651e-04\n",
      "Epoch 53/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4787e-04\n",
      "Epoch 54/100\n",
      "732/732 [==============================] - 14s 20ms/step - loss: 1.4730e-04\n",
      "Epoch 55/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4818e-04\n",
      "Epoch 56/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4659e-04\n",
      "Epoch 57/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4682e-04\n",
      "Epoch 58/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4683e-04\n",
      "Epoch 59/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4668e-04\n",
      "Epoch 60/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4468e-04\n",
      "Epoch 61/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4591e-04\n",
      "Epoch 62/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4557e-04\n",
      "Epoch 63/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.4600e-04\n",
      "Epoch 64/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.4506e-04\n",
      "Epoch 65/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4487e-04\n",
      "Epoch 66/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4600e-04\n",
      "Epoch 67/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4398e-04\n",
      "Epoch 68/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4441e-04\n",
      "Epoch 69/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4671e-04\n",
      "Epoch 70/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4429e-04\n",
      "Epoch 71/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4417e-04\n",
      "Epoch 72/100\n",
      "732/732 [==============================] - 16s 22ms/step - loss: 1.4391e-04\n",
      "Epoch 73/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4562e-04\n",
      "Epoch 74/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4355e-04\n",
      "Epoch 75/100\n",
      "732/732 [==============================] - 15s 21ms/step - loss: 1.4351e-04\n",
      "Epoch 76/100\n",
      "732/732 [==============================] - 18s 24ms/step - loss: 1.4575e-04\n",
      "Epoch 77/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4310e-04\n",
      "Epoch 78/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4524e-04\n",
      "Epoch 79/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4320e-04\n",
      "Epoch 80/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4424e-04\n",
      "Epoch 81/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4430e-04\n",
      "Epoch 82/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4277e-04\n",
      "Epoch 83/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4316e-04\n",
      "Epoch 84/100\n",
      "732/732 [==============================] - 15s 21ms/step - loss: 1.4277e-04\n",
      "Epoch 85/100\n",
      "732/732 [==============================] - 18s 24ms/step - loss: 1.4381e-04\n",
      "Epoch 86/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.4317e-04\n",
      "Epoch 87/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4260e-04\n",
      "Epoch 88/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4229e-04\n",
      "Epoch 89/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4310e-04\n",
      "Epoch 90/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4134e-04\n",
      "Epoch 91/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4233e-04\n",
      "Epoch 92/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4212e-04\n",
      "Epoch 93/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4064e-04\n",
      "Epoch 94/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4218e-04\n",
      "Epoch 95/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4233e-04\n",
      "Epoch 96/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4272e-04\n",
      "Epoch 97/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4126e-04\n",
      "Epoch 98/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4085e-04\n",
      "Epoch 99/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4224e-04\n",
      "Epoch 100/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4101e-04\n",
      "Training loss: 0.00013824041525367647\n",
      "Test loss: 0.0003246361156925559\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load data from Excel file\n",
    "data = pd.read_excel('dataset/dataset.xlsx')\n",
    "\n",
    "# Parse timestamps and set as index\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select relevant features\n",
    "features = ['PQM08 Real Power Mean (kW)', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT01', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT02', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT03', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT04', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT01', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT02']\n",
    "\n",
    "data = data[features]\n",
    "\n",
    "# Preprocess data to remove NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Check for NaN and infinity values\n",
    "print(data.isnull().sum())  # Check for NaN values\n",
    "print(np.any(np.isnan(scaled_data)))  # Check for NaN values after scaling\n",
    "print(np.any(np.isinf(scaled_data)))  # Check for infinity values after scaling\n",
    "\n",
    "# Print first 5 rows of scaled data\n",
    "print(scaled_data[:5])\n",
    "\n",
    "# Determine train data length (e.g., 80% of total data)\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "test_size = len(scaled_data) - train_size\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = scaled_data[0:train_size, :]\n",
    "test_data = scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "# Create dataset for LSTM\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - time_step):\n",
    "        a = dataset[i:(i + time_step), :]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + time_step, 1])  # Select temperature column as target\n",
    "    return np.array(data_X), np.array(data_Y)\n",
    "\n",
    "# Prepare training data for model\n",
    "time_step = 10  # Number of time steps\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input for LSTM [number of samples, time steps, number of features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "# Initialize LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], len(features))))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f'Training loss: {train_loss}')\n",
    "\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# Hasil Model akan digunakan untuk melakukan prediksi early warning peningkatan suhu pada sebuah server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl.metadata (138 kB)\n",
      "     ------------------------------------ 138.5/138.5 kB 822.2 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-win_amd64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "   ---------------------------------------- 7.2/7.2 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 965.4/965.4 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 55.8/55.8 kB 484.6 kB/s eta 0:00:00\n",
      "Downloading Pillow-9.5.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "   -------------------------------------- 103.2/103.2 kB 989.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.5 matplotlib-3.5.3 pillow-9.5.0 pyparsing-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2017.3 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting numpy>=1.17.3 (from pandas)\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "   ---------------------------------------- 10.0/10.0 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading numpy-1.21.6-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "   ---------------------------------------- 14.0/14.0 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ---------------------------------------- 505.5/505.5 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.21.6 pandas-1.3.5 pytz-2024.1\n",
      "Requirement already satisfied: numpy in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (1.21.6)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from scikit-learn) (1.21.6)\n",
      "Collecting scipy>=1.1.0 (from scikit-learn)\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
      "   ---------------------------------------- 7.1/7.1 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 302.2/302.2 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "   ---------------------------------------- 34.1/34.1 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.1.0\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting tensorflow-intel==2.11.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (24.0)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl.metadata (807 bytes)\n",
      "Requirement already satisfied: setuptools in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (47.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp37-cp37m-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading grpcio-1.62.2-cp37-cp37m-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.12,>=2.11 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.12,>=2.11.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading google_auth-2.30.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "   ---------------------------------------- 266.3/266.3 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   -------------------------------------- 133.7/133.7 kB 875.6 kB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 57.5/57.5 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.62.2-cp37-cp37m-win_amd64.whl (4.5 MB)\n",
      "   ---------------------------------------- 4.5/4.5 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 2.6/2.6 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 1.7/1.7 MB 893.5 kB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 26.4/26.4 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 65.5/65.5 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "   ---------------------------------------- 896.6/896.6 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "   ---------------------------------------- 6.0/6.0 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "   ---------------------------------------- 439.2/439.2 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Downloading wrapt-1.16.0-cp37-cp37m-win_amd64.whl (37 kB)\n",
      "Downloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "   -------------------------------------- 193.7/193.7 kB 733.9 kB/s eta 0:00:00\n",
      "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 94.2/94.2 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 62.6/62.6 kB 844.1 kB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "   ---------------------------------------- 781.3/781.3 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 233.6/233.6 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 65.4/65.4 kB 875.2 kB/s eta 0:00:00\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 164.4/164.4 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl (98 kB)\n",
      "   ---------------------------------------- 98.1/98.1 kB 799.0 kB/s eta 0:00:00\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 66.8/66.8 kB 912.8 kB/s eta 0:00:00\n",
      "Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl (17 kB)\n",
      "Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "   -------------------------------------- 181.3/181.3 kB 914.8 kB/s eta 0:00:00\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "   ---------------------------------------- 124.2/124.2 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 151.7/151.7 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 84.9/84.9 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, zipp, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, MarkupSafe, keras, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, importlib-metadata, astunparse, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.6.2 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.30.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.62.2 h5py-3.8.0 idna-3.7 importlib-metadata-6.7.0 keras-2.11.0 libclang-18.1.1 markdown-3.4.4 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.7.1 urllib3-2.0.7 werkzeug-2.2.3 wheel-0.42.0 wrapt-1.16.0 zipp-3.15.0\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.3-py2.py3-none-any.whl (251 kB)\n",
      "   ---------------------------------------- 251.3/251.3 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.3\n",
      "Collecting flask\n",
      "  Downloading Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from flask) (2.2.3)\n",
      "Collecting Jinja2>=3.0 (from flask)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting itsdangerous>=2.0 (from flask)\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click>=8.0 (from flask)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from flask) (6.7.0)\n",
      "Requirement already satisfied: colorama in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from click>=8.0->flask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from importlib-metadata>=3.6.0->flask) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from importlib-metadata>=3.6.0->flask) (4.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dokumen\\edp\\angga\\lstm\\.venv\\lib\\site-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
      "Downloading Flask-2.2.5-py3-none-any.whl (101 kB)\n",
      "   -------------------------------------- 101.8/101.8 kB 654.1 kB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 97.9/97.9 kB 804.6 kB/s eta 0:00:00\n",
      "Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "   -------------------------------------- 133.3/133.3 kB 984.2 kB/s eta 0:00:00\n",
      "Installing collected packages: Jinja2, itsdangerous, click, flask\n",
      "Successfully installed Jinja2-3.1.4 click-8.1.7 flask-2.2.5 itsdangerous-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "!pip install openpyxl\n",
    "!pip install flask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
