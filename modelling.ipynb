{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQM08 Real Power Mean (kW)       0\n",
      "RmTmp - DH01_ROW01_LA_TTHT01     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT02     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT03     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT04     0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT01    0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT02    0\n",
      "dtype: int64\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load data from Excel file\n",
    "data = pd.read_excel('dataset/dataset.xlsx')\n",
    "\n",
    "# Parse timestamps and set as index\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Select relevant features\n",
    "features = ['PQM08 Real Power Mean (kW)', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT01', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT02', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT03', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT04', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT01', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT02']\n",
    "\n",
    "data = data[features]\n",
    "\n",
    "# preprocess data to remove NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "print(data.isnull().sum())  # Untuk mengecek jumlah nilai NaN\n",
    "print(np.any(np.isnan(scaled_data)))  # Untuk mengecek apakah ada nilai NaN setelah normalisasi\n",
    "print(np.any(np.isinf(scaled_data)))  # Untuk mengecek apakah ada nilai infinity setelah \n",
    "\n",
    "\n",
    "# Print first 5 rows of scaled data\n",
    "print(scaled_data[:5])\n",
    "\n",
    "# Tentukan panjang data latih (misalnya 80% dari total data)\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "test_size = len(scaled_data) - train_size\n",
    "\n",
    "# Bagi data menjadi data latih dan data uji\n",
    "train_data = scaled_data[0:train_size, :]\n",
    "test_data = scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "# Langkah 2: Memformat Data untuk LSTM\n",
    "# LSTM memerlukan data yang diformat dalam bentuk tiga dimensi: (jumlah sampel, jumlah time steps, jumlah fitur).\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), :]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + time_step, 1])  # Memilih kolom suhu sebagai target\n",
    "    return np.array(data_X), np.array(data_Y)\n",
    "\n",
    "# Menyiapkan data latih untuk model\n",
    "time_step = 10  # Jumlah time steps\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input untuk LSTM [jumlah sampel, time steps, jumlah fitur]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "# Inisialisasi model LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], len(features))))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Kompilasi model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Melatih model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluasi model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f'Training loss: {train_loss}')\n",
    "\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQM08 Real Power Mean (kW)       0\n",
      "RmTmp - DH01_ROW01_LA_TTHT01     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT02     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT03     0\n",
      "RmTmp - DH01_ROW01_LA_TTHT04     0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT01    0\n",
      "RmRhTL - DH01_ROW01_LA_TTHT02    0\n",
      "dtype: int64\n",
      "False\n",
      "False\n",
      "[[0.34869488 0.14371257 0.08985025 0.05882353 0.34660422 0.27756131\n",
      "  0.29310734]\n",
      " [0.35183535 0.14371257 0.08153078 0.05294118 0.34660422 0.27549851\n",
      "  0.2980791 ]\n",
      " [0.34450742 0.13972056 0.08485857 0.0745098  0.34660422 0.27045611\n",
      "  0.28497175]\n",
      " [0.34964632 0.16167665 0.08985025 0.0745098  0.34660422 0.2622049\n",
      "  0.28610169]\n",
      " [0.36538433 0.14371257 0.08485857 0.06862745 0.34192037 0.26953931\n",
      "  0.29220339]]\n",
      "Epoch 1/100\n",
      "732/732 [==============================] - 18s 15ms/step - loss: 4.3872e-04\n",
      "Epoch 2/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.9559e-04\n",
      "Epoch 3/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.8543e-04\n",
      "Epoch 4/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.8191e-04\n",
      "Epoch 5/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.7811e-04\n",
      "Epoch 6/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.7679e-04\n",
      "Epoch 7/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.7019e-04\n",
      "Epoch 8/100\n",
      "732/732 [==============================] - 17s 23ms/step - loss: 1.6748e-04\n",
      "Epoch 9/100\n",
      "732/732 [==============================] - 19s 27ms/step - loss: 1.6832e-04\n",
      "Epoch 10/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.6561e-04\n",
      "Epoch 11/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.6474e-04\n",
      "Epoch 12/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.6507e-04\n",
      "Epoch 13/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.6341e-04\n",
      "Epoch 14/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.5966e-04\n",
      "Epoch 15/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.6383e-04\n",
      "Epoch 16/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.6016e-04\n",
      "Epoch 17/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.5893e-04\n",
      "Epoch 18/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5844e-04\n",
      "Epoch 19/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.5687e-04\n",
      "Epoch 20/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5835e-04\n",
      "Epoch 21/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5631e-04\n",
      "Epoch 22/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5442e-04\n",
      "Epoch 23/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.5536e-04\n",
      "Epoch 24/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5583e-04\n",
      "Epoch 25/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5296e-04\n",
      "Epoch 26/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5123e-04\n",
      "Epoch 27/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5385e-04\n",
      "Epoch 28/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5015e-04\n",
      "Epoch 29/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.5327e-04\n",
      "Epoch 30/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5163e-04\n",
      "Epoch 31/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.5188e-04\n",
      "Epoch 32/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.5172e-04\n",
      "Epoch 33/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4922e-04\n",
      "Epoch 34/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.5065e-04\n",
      "Epoch 35/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.5291e-04\n",
      "Epoch 36/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4966e-04\n",
      "Epoch 37/100\n",
      "732/732 [==============================] - 10s 14ms/step - loss: 1.4971e-04\n",
      "Epoch 38/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.5294e-04\n",
      "Epoch 39/100\n",
      "732/732 [==============================] - 10s 14ms/step - loss: 1.4862e-04\n",
      "Epoch 40/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4938e-04\n",
      "Epoch 41/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4932e-04\n",
      "Epoch 42/100\n",
      "732/732 [==============================] - 10s 14ms/step - loss: 1.4955e-04\n",
      "Epoch 43/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4819e-04\n",
      "Epoch 44/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.4777e-04\n",
      "Epoch 45/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4930e-04\n",
      "Epoch 46/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4841e-04\n",
      "Epoch 47/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4774e-04\n",
      "Epoch 48/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4666e-04\n",
      "Epoch 49/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4929e-04\n",
      "Epoch 50/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4623e-04\n",
      "Epoch 51/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4777e-04\n",
      "Epoch 52/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4651e-04\n",
      "Epoch 53/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4787e-04\n",
      "Epoch 54/100\n",
      "732/732 [==============================] - 14s 20ms/step - loss: 1.4730e-04\n",
      "Epoch 55/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4818e-04\n",
      "Epoch 56/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4659e-04\n",
      "Epoch 57/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4682e-04\n",
      "Epoch 58/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4683e-04\n",
      "Epoch 59/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4668e-04\n",
      "Epoch 60/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4468e-04\n",
      "Epoch 61/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4591e-04\n",
      "Epoch 62/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4557e-04\n",
      "Epoch 63/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.4600e-04\n",
      "Epoch 64/100\n",
      "732/732 [==============================] - 11s 16ms/step - loss: 1.4506e-04\n",
      "Epoch 65/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4487e-04\n",
      "Epoch 66/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4600e-04\n",
      "Epoch 67/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4398e-04\n",
      "Epoch 68/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4441e-04\n",
      "Epoch 69/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4671e-04\n",
      "Epoch 70/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4429e-04\n",
      "Epoch 71/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4417e-04\n",
      "Epoch 72/100\n",
      "732/732 [==============================] - 16s 22ms/step - loss: 1.4391e-04\n",
      "Epoch 73/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4562e-04\n",
      "Epoch 74/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4355e-04\n",
      "Epoch 75/100\n",
      "732/732 [==============================] - 15s 21ms/step - loss: 1.4351e-04\n",
      "Epoch 76/100\n",
      "732/732 [==============================] - 18s 24ms/step - loss: 1.4575e-04\n",
      "Epoch 77/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4310e-04\n",
      "Epoch 78/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4524e-04\n",
      "Epoch 79/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4320e-04\n",
      "Epoch 80/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4424e-04\n",
      "Epoch 81/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4430e-04\n",
      "Epoch 82/100\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 1.4277e-04\n",
      "Epoch 83/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4316e-04\n",
      "Epoch 84/100\n",
      "732/732 [==============================] - 15s 21ms/step - loss: 1.4277e-04\n",
      "Epoch 85/100\n",
      "732/732 [==============================] - 18s 24ms/step - loss: 1.4381e-04\n",
      "Epoch 86/100\n",
      "732/732 [==============================] - 15s 20ms/step - loss: 1.4317e-04\n",
      "Epoch 87/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4260e-04\n",
      "Epoch 88/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4229e-04\n",
      "Epoch 89/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4310e-04\n",
      "Epoch 90/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4134e-04\n",
      "Epoch 91/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4233e-04\n",
      "Epoch 92/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4212e-04\n",
      "Epoch 93/100\n",
      "732/732 [==============================] - 12s 16ms/step - loss: 1.4064e-04\n",
      "Epoch 94/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4218e-04\n",
      "Epoch 95/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4233e-04\n",
      "Epoch 96/100\n",
      "732/732 [==============================] - 13s 17ms/step - loss: 1.4272e-04\n",
      "Epoch 97/100\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 1.4126e-04\n",
      "Epoch 98/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4085e-04\n",
      "Epoch 99/100\n",
      "732/732 [==============================] - 11s 15ms/step - loss: 1.4224e-04\n",
      "Epoch 100/100\n",
      "732/732 [==============================] - 12s 17ms/step - loss: 1.4101e-04\n",
      "Training loss: 0.00013824041525367647\n",
      "Test loss: 0.0003246361156925559\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load data from Excel file\n",
    "data = pd.read_excel('dataset/dataset.xlsx')\n",
    "\n",
    "# Parse timestamps and set as index\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select relevant features\n",
    "features = ['PQM08 Real Power Mean (kW)', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT01', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT02', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT03', \n",
    "            'RmTmp - DH01_ROW01_LA_TTHT04', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT01', \n",
    "            'RmRhTL - DH01_ROW01_LA_TTHT02']\n",
    "\n",
    "data = data[features]\n",
    "\n",
    "# Preprocess data to remove NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Check for NaN and infinity values\n",
    "print(data.isnull().sum())  # Check for NaN values\n",
    "print(np.any(np.isnan(scaled_data)))  # Check for NaN values after scaling\n",
    "print(np.any(np.isinf(scaled_data)))  # Check for infinity values after scaling\n",
    "\n",
    "# Print first 5 rows of scaled data\n",
    "print(scaled_data[:5])\n",
    "\n",
    "# Determine train data length (e.g., 80% of total data)\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "test_size = len(scaled_data) - train_size\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = scaled_data[0:train_size, :]\n",
    "test_data = scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "# Create dataset for LSTM\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - time_step):\n",
    "        a = dataset[i:(i + time_step), :]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + time_step, 1])  # Select temperature column as target\n",
    "    return np.array(data_X), np.array(data_Y)\n",
    "\n",
    "# Prepare training data for model\n",
    "time_step = 10  # Number of time steps\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input for LSTM [number of samples, time steps, number of features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))\n",
    "\n",
    "# Initialize LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], len(features))))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f'Training loss: {train_loss}')\n",
    "\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# Hasil Model akan digunakan untuk melakukan prediksi early warning peningkatan suhu pada sebuah server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save the model\n",
    "model.save('FlaskApp/lstm_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
